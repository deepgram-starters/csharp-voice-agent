<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deepgram Voice Agent</title>
</head>
<body>
    <h1>Deepgram Voice Agent Browser Demo</h1>
    <p>This is a demo of the Deepgram Voice Agent. It uses the <a href="https://developers.deepgram.com/reference/voice-agent-api/agent">Deepgram Voice Agent API</a>.</p>
    <p>Please enable your microphone in the browser to start the conversation.</p>
    <script>
        let websocket;
        let mediaStream;
        let audioContext;
        let processor;
        let isConnected = false;
        let audioQueue = []; // Queue for managing incoming audio chunks
        let isPlaying = false; // Flag to track if we're currently playing audio
        let selectedDeviceId;

        async function init() {
            try {
                // Create audio context early
                audioContext = new AudioContext({
                    sampleRate: 16000 // Match the Deepgram sample rate
                });

                // Get microphone permission with specific constraints
                const constraints = {
                    audio: {
                        deviceId: selectedDeviceId ? { exact: selectedDeviceId } : undefined,
                        channelCount: 1,
                        sampleRate: 16000,
                        echoCancellation: false,  // Can be toggled
                        noiseSuppression: false,  // Can be toggled
                        autoGainControl: false,   // Can be toggled
                        latency: 0,              // Minimize latency
                        // Advanced constraints for better control
                        googEchoCancellation: false,
                        googAutoGainControl: false,
                        googNoiseSuppression: false,
                        googHighpassFilter: true
                    }
                };
                mediaStream = await navigator.mediaDevices.getUserMedia(constraints);

                // Connect to WebSocket server
                const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                const wsUrl = `${protocol}//${window.location.host}/ws`;
                console.log('Attempting to connect to WebSocket:', wsUrl);
                websocket = new WebSocket(wsUrl);

                websocket.onopen = () => {
                    console.log('Connected to server');
                    isConnected = true;
                    startStreaming();
                };

                websocket.onclose = (event) => {
                    console.log('Disconnected from server', event.code, event.reason);
                    isConnected = false;
                    stopStreaming();
                };

                websocket.onerror = (error) => {
                    console.error('WebSocket error:', error);
                    console.error('WebSocket readyState:', websocket.readyState);
                    console.error('WebSocket URL:', wsUrl);
                };

                websocket.onmessage = (event) => {
                    try {
                        const data = JSON.parse(event.data);
                        console.log('Received message:', data);

                        if (data.type === 'audio') {
                            console.log('Audio data received:', data.data.length, 'bytes');
                            // Decode base64 audio data
                            const audioBytes = atob(data.data);
                            const audioData = new Int16Array(audioBytes.length / 2);
                            for (let i = 0; i < audioBytes.length; i += 2) {
                                audioData[i / 2] = (audioBytes.charCodeAt(i) & 0xff) |
                                                 ((audioBytes.charCodeAt(i + 1) & 0xff) << 8);
                            }
                            audioQueue.push(audioData);
                            if (!isPlaying) {
                                playNextInQueue();
                            }
                        } else if (data.type === 'text') {
                            console.log('Agent text:', data.data);
                        }
                    } catch (error) {
                        console.error('Error parsing message:', error);
                    }
                };

            } catch (error) {
                console.error('Error initializing:', error);
            }
        }

        function startStreaming() {
            if (!mediaStream || !isConnected) return;

            console.log('Starting audio streaming...');
            console.log('MediaStream tracks:', mediaStream.getTracks().length);
            console.log('WebSocket readyState:', websocket.readyState);

            try {
                const source = audioContext.createMediaStreamSource(mediaStream);

                // Create a worklet for better audio processing
                const bufferSize = 2048;
                processor = audioContext.createScriptProcessor(bufferSize, 1, 1);

                source.connect(processor);
                processor.connect(audioContext.destination);

                let lastSendTime = 0;
                const sendInterval = 100; // Send every 100ms

                processor.onaudioprocess = (e) => {
                    console.log('Audio process callback fired');
                    const now = Date.now();
                    if (websocket?.readyState === WebSocket.OPEN && now - lastSendTime >= sendInterval) {
                        const inputData = e.inputBuffer.getChannelData(0);
                        const pcmData = convertFloatToPcm(inputData);

                        // Check if there's actual audio data (lower threshold)
                        const hasAudio = pcmData.some(sample => Math.abs(sample) > 10);

                        // Always log the first few samples to see what we're getting
                        if (pcmData.length > 0) {
                            console.log('Audio samples:', pcmData.slice(0, 5), 'hasAudio:', hasAudio);
                        }

                        if (hasAudio) {
                            console.log('Sending audio data:', {
                                samples: pcmData.length,
                                sampleRate: audioContext.sampleRate,
                                interval: now - lastSendTime,
                                hasAudio: true
                            });
                        }

                        // Convert to base64 and send as JSON
                        const base64Audio = btoa(String.fromCharCode(...new Uint8Array(pcmData.buffer)));
                        const message = {
                            type: 'audio',
                            data: base64Audio
                        };
                        websocket.send(JSON.stringify(message));
                        lastSendTime = now;
                    }
                };

                console.log('Audio streaming started successfully');
            } catch (error) {
                console.error('Error starting audio stream:', error);
            }
        }

        function convertFloatToPcm(floatData) {
            const pcmData = new Int16Array(floatData.length);
            for (let i = 0; i < floatData.length; i++) {
                const s = Math.max(-1, Math.min(1, floatData[i]));
                pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }
            return pcmData;
        }

        async function playNextInQueue() {
            if (audioQueue.length === 0) {
                isPlaying = false;
                return;
            }

            isPlaying = true;
            const audioData = audioQueue.shift();

            try {
                // Ensure audio context is running
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }

                // Create buffer with correct sample rate for agent's audio (16000Hz)
                const buffer = audioContext.createBuffer(1, audioData.length, 16000);
                const channelData = buffer.getChannelData(0);

                // Convert Int16 to Float32 with proper scaling
                for (let i = 0; i < audioData.length; i++) {
                    // Normalize to [-1, 1] range
                    channelData[i] = audioData[i] / 32768.0;
                }

                // Create and configure source
                const source = audioContext.createBufferSource();
                source.buffer = buffer;

                // Connect directly to destination for lower latency
                source.connect(audioContext.destination);

                // Handle playback completion
                source.onended = () => {
                    playNextInQueue(); // Play next chunk when current one ends
                };

                // Start playback immediately
                source.start(0);
            } catch (error) {
                console.error('Error playing audio:', error);
                isPlaying = false;
                playNextInQueue(); // Try next chunk if current fails
            }
        }

        function stopStreaming() {
            audioQueue = []; // Clear audio queue
            isPlaying = false;
            if (processor) {
                processor.disconnect();
                processor = null;
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            isConnected = false;
        }

        // Initialize when the page loads
        window.onload = init;

        // Clean up when the page is closed
        window.onbeforeunload = () => {
            stopStreaming();
            if (websocket) {
                websocket.close();
            }
        };
    </script>
</body>
</html>